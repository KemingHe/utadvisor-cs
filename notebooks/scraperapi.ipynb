{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0f59f9-fc2a-4de8-8add-594d28e0a8af",
   "metadata": {},
   "source": [
    "Given a domain, i.e. cs.utexas.edu , scrape and generate a site.json\n",
    "containing all text content of all pages available through \n",
    "domain/sitemap.xml.\n",
    "\n",
    "1. Get list of sites (and metadata) from sitemap.\n",
    "2. Interate through sites, scrape text content.\n",
    "3. Store text content, scrape_timestamp \n",
    "together with site metadata to output list of site objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1233acd-cda6-4dac-8b97-0bc170b4baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env imports.\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Retrieve the API keys from environment variables\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY: str = getenv('SCRAPERAPI_API_KEY')\n",
    "NUM_RETRIES: int = int(getenv('SCRAPERAPI_NUM_RETRIES'))\n",
    "MAX_WORKERS: int = int(getenv('SCRAPERAPI_MAX_WORKERS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc6990-419d-4179-83b1-be28931e7b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from scraperapi_sdk import ScraperAPIClient\n",
    "from scraperapi_sdk.exceptions import ScraperAPIException\n",
    "\n",
    "client: ScraperAPIClient = ScraperAPIClient(API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3780a127-cc09-4538-bcfb-8803c0fe0975",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from lxml import etree\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "SITEMAP_NAMESPACE = {'s': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "\n",
    "class EmptySitemapDataException(Exception):\n",
    "    '''No valid urls found in the requested domain's sitemap.xml.'''\n",
    "    pass\n",
    "\n",
    "def get_subfield(url_tag: etree._Element, field_name: str) -> Optional[str]:\n",
    "    field = url_tag.find(f's:{field_name}', SITEMAP_NAMESPACE)\n",
    "    return field.text if field is not None else None\n",
    "\n",
    "def get_sitemap_data(client: ScraperAPIClient, sitemap_url: str) -> List[Dict[str, str]]:\n",
    "    # Use 'make_request' instead of 'get' for byte str compatibility.\n",
    "    # i.e. w/ encoding declaration <?xml version=\"1.0\" encoding=\"UTF-8\"?> \n",
    "    response = client.make_request(url=sitemap_url, params={'retry': NUM_RETRIES})\n",
    "    root = etree.fromstring(response.content)\n",
    "    sitemap_data = []\n",
    "\n",
    "    for url in root.findall('.//s:url', SITEMAP_NAMESPACE):\n",
    "        loc = url.find('s:loc', SITEMAP_NAMESPACE)\n",
    "        if loc is not None:\n",
    "            url_data = {\n",
    "                'loc': loc.text,\n",
    "                'lastmod': get_subfield(url_tag=url, field_name='lastmod'),\n",
    "                'priority': get_subfield(url_tag=url, field_name='priority'),\n",
    "                'changefreq': get_subfield(url_tag=url, field_name='changefreq')\n",
    "            }\n",
    "            sitemap_data.append(url_data)\n",
    "            # pprint(f'Success: Added URL data to sitemap dataset: {url_data}.')\n",
    "        else:\n",
    "            pprint(f'Warning: No <loc> tag found in URL data: {url}. Skipping...')\n",
    "    \n",
    "    if not sitemap_data:\n",
    "        raise EmptySitemapDataException\n",
    "\n",
    "    return sitemap_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f27052-a2f0-4347-9c82-751cfd6afb3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(get_sitemap_data(client=client, sitemap_url='https://www.cs.utexas.edu/sitemap.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a061fa5-7a36-4564-9159-24637a00ce02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(get_sitemap_data(client=client, sitemap_url='https://oia.osu.edu/sitemap.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a258a92-3e65-4ad6-8745-076013df5d4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class EmptyTextContentException(Exception):\n",
    "    '''No text content found at the requested URL.'''\n",
    "    pass\n",
    "\n",
    "def get_text_content(client: ScraperAPIClient, url: str) -> str:\n",
    "    raw_html = client.get(url, params={'retry': NUM_RETRIES})\n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "    # Parse the response with BeautifulSoup and extract all text content,\n",
    "    # excluding <header> and <footer> tags if present.\n",
    "    soup.header and soup.header.decompose()\n",
    "    soup.footer and soup.footer.decompose()\n",
    "    text_content = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "    if not text_content:\n",
    "        raise EmptyTextContentException\n",
    "    \n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd85298-a287-406c-bbfd-59ef25ad6e0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(get_text_content(client=client, url='https://www.cs.utexas.edu/about'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93562f3f-95a0-4c91-90c1-4b388ec368fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, ParseResult\n",
    "\n",
    "def needs_sanitation(loc: str, domain: str) -> bool:\n",
    "    '''Verifies loc URL uses https and matches the domain.'''\n",
    "    parsed_loc: ParseResult = urlparse(loc)\n",
    "    return parsed_loc.scheme != 'https' or parsed_loc.hostname != f'www.{domain}'\n",
    "\n",
    "def sanitize_loc(loc: str, domain: str) -> str:\n",
    "    # Init with secure https domain base.\n",
    "    sanitized_loc: str = f'https://www.{domain}'\n",
    "\n",
    "    # Parse (instead of regexp) the loc URL to extract path.\n",
    "    parsed_loc: ParseResult = urlparse(loc)\n",
    "\n",
    "    # Append the path, query, and fragment (if present) to the sanitized URL.\n",
    "    sanitized_loc += parsed_loc.path\n",
    "    if parsed_loc.query:\n",
    "        sanitized_loc += f'?{parsed_loc.query}'\n",
    "    if parsed_loc.fragment:\n",
    "        sanitized_loc += f'#{parsed_loc.fragment}'\n",
    "        \n",
    "    return sanitized_loc\n",
    "    \n",
    "\n",
    "def sanitize_sitemap_data(sitemap_data: List[Dict[str, str]], domain: str) -> List[Dict[str, str]]:\n",
    "    processed_sitemap_data = []\n",
    "    for data in sitemap_data:\n",
    "        if data.get('loc'):\n",
    "            if needs_sanitation(loc=data['loc'], domain=domain):\n",
    "                sanitized_data = { **data, 'loc': sanitize_loc(loc = data['loc'], domain=domain) }\n",
    "                processed_sitemap_data.append(sanitized_data)\n",
    "            else:\n",
    "                processed_sitemap_data.append(data)\n",
    "\n",
    "    return processed_sitemap_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32127141-cce8-496d-8955-941bba33eb62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "domain = 'oia.osu.edu'\n",
    "sitemap_url = f'https://www.{domain}/sitemap.xml'\n",
    "sitemap_data = get_sitemap_data(client, sitemap_url)\n",
    "sanitized_sitemap_data = sanitize_sitemap_data(sitemap_data, domain)\n",
    "\n",
    "pprint(sanitized_sitemap_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c40d28-d6fc-4b73-a929-72cad27dd241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "\n",
    "def get_lastmod_timestamp(data: Dict[str, str]) -> Optional[str]:\n",
    "    lastmod: str = data.get('lastmod')\n",
    "    # Validate lastmod is a non-empty field.\n",
    "    if lastmod:\n",
    "        try:\n",
    "            # Validate lastmod is a valid iso time str.\n",
    "            datetime.fromisoformat(lastmod)\n",
    "            return lastmod\n",
    "        except ValueError:\n",
    "            pass\n",
    "    # Explicitly return None if any validation fails.\n",
    "    return None\n",
    "\n",
    "def scrape_domain_from_sitemap_data(\n",
    "    client: ScraperAPIClient,\n",
    "    sitemap_url: str,\n",
    "    sitemap_data: List[Dict[str, str]], \n",
    ") -> (List[Dict[str, str]], List[Dict[str, str]]):\n",
    "\n",
    "    lock = Lock()\n",
    "    domain_data: List[Dict[str, str]] = []\n",
    "    failure_sites: List[Dict[str, str]] = []\n",
    "    \n",
    "    def process_url(data: Dict[str, str]):\n",
    "        try:\n",
    "            if 'loc' not in data:\n",
    "                raise KeyError('The key 'loc' is missing in sitemap data item.')\n",
    "\n",
    "            url = data['loc']\n",
    "            text_content: str = get_text_content(client, url)\n",
    "            scrape_timestamp: str = datetime.now().isoformat()\n",
    "            lastmod_timestamp: str = get_lastmod_timestamp(data) or scrape_timestamp\n",
    "            \n",
    "            site_data = {\n",
    "                'url': url,\n",
    "                'src': sitemap_url,\n",
    "                'text_content': text_content,\n",
    "                'lastmod_timestamp': lastmod_timestamp,\n",
    "                'scrape_timestamp': scrape_timestamp,\n",
    "            }\n",
    "            \n",
    "            with lock:\n",
    "                domain_data.append(site_data)\n",
    "                \n",
    "        except (\n",
    "            KeyError,\n",
    "            ScraperAPIException,\n",
    "            EmptySitemapDataException,\n",
    "            EmptyTextContentException\n",
    "        ) as e:\n",
    "            with lock:\n",
    "                failure_sites.append(data)\n",
    "            print(e)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        executor.map(process_url, sitemap_data)\n",
    "\n",
    "    print(f'Successfully scraped text content from {len(domain_data)} out of {len(sitemap_data)} total sites provided.')\n",
    "    return domain_data, failure_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8501aba2-95d7-47cb-97a0-589b86a796f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "domain: str = 'cs.utexas.edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95937e55-1502-4c42-ad22-7af7bf47fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain: str = 'aede.osu.edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db4ec6b-76bb-4d57-bee4-54c8d9da0ef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "domain: str = 'oia.osu.edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c7a26-551b-4999-ab18-4b69c58367f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sitemap_url: str = f'https://www.{domain}/sitemap.xml'\n",
    "sitemap_data: List[Dict[str, str]] = get_sitemap_data(client, sitemap_url)\n",
    "sanitized_sitemap_data: List[Dict[str, str]] = sanitize_sitemap_data(sitemap_data, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb05473-5611-4db6-a363-c199de51c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_data, failure_sites = scrape_domain_from_sitemap_data(client, sitemap_url, sanitized_sitemap_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19afc6fe-fd1b-4891-8303-d227de52803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_data, additional_failure_sites = scrape_domain_from_sitemap_data(client, sitemap_url, failure_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da0101-0517-4202-8e3a-d8e93e9bd7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_data.append(additional_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25597972-0b87-46f5-a758-2d63a8525d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_output_filename(domain: str) -> str:\n",
    "    return f'{domain.replace('.', '_')}_domain_data.json'\n",
    "\n",
    "print(prepare_output_filename(domain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace3b363-d9ff-4834-ab06-0f546f87ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_filename = prepare_output_filename(domain)\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(domain_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c737068-c35a-46e1-a8b3-a07aced2f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556bde1b-f83e-4263-b462-480c3f403350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(domain_data)\n",
    "df['text_length'] = df['text_content'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e7300-e169-4bfd-b16a-0414e241b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    df, \n",
    "    x='text_length', \n",
    "    title='Distribution of Text Content Length',\n",
    "    labels={'text_length': 'Text Length (chars)'},\n",
    "    nbins=5000  # Adjust number of bins as needed\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"iframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538c14c-845f-437d-bb3e-9d32c5a196a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
