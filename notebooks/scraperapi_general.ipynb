{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0f59f9-fc2a-4de8-8add-594d28e0a8af",
   "metadata": {},
   "source": [
    "# General Scraper\n",
    "\n",
    "Given a domain, breadth-first crawl and generate a domain_data.json\n",
    "containing all text content of all pages available through crawling, \n",
    "useful when sitemap.xml is not available.\n",
    "\n",
    "## Shared Variables\n",
    "\n",
    "* domain (i.e. uscis.gov)\n",
    "* processed_set # set of all crawled sites\n",
    "* crawl_queue # list of sites to crawl\n",
    "* domain_data # list of processed SiteData\n",
    "* crawl_site # internal function to crawl the next site in the queue\n",
    "\n",
    "## Helper Functions\n",
    "\n",
    "```python\n",
    "def extract_text_content(html: str) -> str:\n",
    "    '''Extracts main text content from HTML, excluding header and footer sections.'''\n",
    "    pass\n",
    "    \n",
    "def extract_links(html: str) -> List[str]:\n",
    "    '''Extracts all links from HTML without filtering.'''\n",
    "    pass\n",
    "\n",
    "def normalize_link(link: str, base_url: str) -> str:\n",
    "    '''Normalizes a single link to a standard format.'''\n",
    "    pass\n",
    "\n",
    "def normalize_links(links: List[str], base_url: str) -> List[str]:\n",
    "    '''Normalizes a list of links to a standard format and removes duplicates.'''\n",
    "    prepared_links = [normalize_link(link) for link in links]\n",
    "    return list(set(prepared_links))\n",
    "\n",
    "def filter_domain_links(links: List[str], domain: str) -> List[str]:\n",
    "    '''Filters links to include only those under the specified domain.'''\n",
    "    pass\n",
    "\n",
    "def filter_new_links(\n",
    "    links: List[str], \n",
    "    in_process_links: Set[str], \n",
    "    processed_links: Set[str]\n",
    ") -> List[str]:\n",
    "    '''Filters links to exclude those that have already been processed.'''\n",
    "    pass\n",
    "```\n",
    "\n",
    "## Additional Concerns\n",
    "\n",
    "* ~Enable JS rendering~ (js render, just like premium, requires 10 credits instead of 1)\n",
    "* Add 100ms delay to thread scheduler\n",
    "* Failure url list to output and rescrape if needed\n",
    "* Parse PDF links using LangChain\n",
    "* Given domain url, scrape one level down non-domain urls on the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1233acd-cda6-4dac-8b97-0bc170b4baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env imports.\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Retrieve the API keys from environment variables\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY: str = getenv('SCRAPERAPI_API_KEY')\n",
    "NUM_RETRIES: int = int(getenv('SCRAPERAPI_NUM_RETRIES'))\n",
    "MAX_WORKERS: int = int(getenv('SCRAPERAPI_MAX_WORKERS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ea44e4-b84d-4016-af5b-1555b60e47c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from typing import Dict, List, Optional, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7c31a-8e19-4def-942b-6ff95da73be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraperapi_sdk import ScraperAPIClient\n",
    "from scraperapi_sdk.exceptions import ScraperAPIException\n",
    "\n",
    "client: ScraperAPIClient = ScraperAPIClient(API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a258a92-3e65-4ad6-8745-076013df5d4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class EmptyTextContentException(Exception):\n",
    "    '''No text content found at the requested URL.'''\n",
    "    pass\n",
    "\n",
    "def extract_text_content(html: str) -> str:\n",
    "    '''Extracts main text content from HTML, excluding header and footer sections.'''\n",
    "    \n",
    "    soup: BeautifulSoup = BeautifulSoup(html, 'html.parser')\n",
    "    soup.header and soup.header.decompose()\n",
    "    soup.footer and soup.footer.decompose()\n",
    "    text_content: str = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "    if not text_content:\n",
    "        raise EmptyTextContentException\n",
    "    \n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c92c5fe-eb24-437b-a01e-6c09806457bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "domain: str = 'osu.dev'\n",
    "url: str = f'https://{domain}'\n",
    "html: str = client.get(url, params={'retry': NUM_RETRIES})\n",
    "text_content: str = extract_text_content(html)\n",
    "print(len(text_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a8e3e-56a6-40ae-9039-a431b831dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e6233-f6fc-4b18-abcd-4c1462b7180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(html: str) -> List[str]:\n",
    "    '''Extracts all links from HTML without filtering.'''\n",
    "    \n",
    "    soup: BeautifulSoup = BeautifulSoup(html, 'html.parser')\n",
    "    links: List[str] = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        links.append(a_tag['href'])\n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d38933-76ac-44a4-a416-6a7d21933962",
   "metadata": {},
   "outputs": [],
   "source": [
    "links: List[str] = extract_links(html)\n",
    "pprint(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04815908-db2c-4b4f-8692-ee94f0e54263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "def normalize_link(link: str, base_url: str) -> Optional[str]:\n",
    "    '''Normalizes a single link to a standard format.'''\n",
    "\n",
    "    result: str = link.strip().strip('/')\n",
    "    if not result or result.startswith('#'):\n",
    "        return None\n",
    "\n",
    "    return urljoin(base_url, result).lower()\n",
    "\n",
    "def normalize_links(links: List[str], base_url: str) -> List[str]:\n",
    "    '''Normalizes a list of links to a standard format and removes duplicates.'''\n",
    "\n",
    "    prepared_links: Set[str] = set()\n",
    "    for link in links:\n",
    "        result: Optional[str] = normalize_link(link, base_url)\n",
    "        if result:\n",
    "            prepared_links.add(result)\n",
    "    \n",
    "    return list(prepared_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8dfe69-cce1-423c-9f6a-629242f4d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_links: List[str] = normalize_links(links, base_url=url)\n",
    "pprint(normalized_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbaddc0-fc12-4e10-8031-a351740e7d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def filter_domain_links(links: List[str], domain: str) -> List[str]:\n",
    "    '''Filters links to include only those under the specified domain.'''\n",
    "\n",
    "    domain_links: List[str] = []\n",
    "    for link in links:\n",
    "        parsed_url: str = urlparse(link)\n",
    "        link_hostname: str = parsed_url.hostname\n",
    "        if link_hostname and link_hostname == domain:\n",
    "            domain_links.append(link)\n",
    "    \n",
    "    return domain_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b588fb-a7a8-4c97-b938-a8c03148e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_links: List[str] = filter_domain_links(links=normalized_links, domain=domain)\n",
    "pprint(domain_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc16544a-dbd5-4c4e-b86b-8fe8bf2c6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_new_links(\n",
    "    links: List[str],\n",
    "    in_progress_set: Set[str],\n",
    "    processed_set: Set[str]\n",
    ") -> List[str]:\n",
    "    '''Filters links to exclude those that are in progress or already processed.'''\n",
    "\n",
    "    return [\n",
    "        link for link in links\n",
    "        if link not in in_progress_set and link not in processed_set\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee0ef5-d77a-4b5b-95a6-c514a6464f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_set: Set[str] = set(['https://osu.dev/projects'])\n",
    "processed_set: Set[str] = set(['https://osu.dev/support'])\n",
    "new_links: List[str] = filter_new_links(domain_links, in_progress_set, processed_set)\n",
    "pprint(new_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028e252-0475-43b3-8885-416627a8a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_new_links(html: str, domain: str, base_url: str, in_progress_set: Set[str], processed_set: Set[str]):\n",
    "    raw_links: List[str] = extract_links(html)\n",
    "    normalized_links: List[str] = normalize_links(raw_links, base_url)\n",
    "    domain_links: List[str] = filter_domain_links(normalized_links, domain)\n",
    "    new_links: List[str] = filter_new_links(domain_links, in_progress_set, processed_set)\n",
    "    return new_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a856ed-d5e8-4b69-9623-e09609aa61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def is_html_response(response) -> bool:\n",
    "    '''Determine if the response content is HTML.'''\n",
    "    # Primary check for Content-Type\n",
    "    if 'Content-Type' in response.headers and response.headers['Content-Type'].startswith('text/html'):\n",
    "        return True\n",
    "\n",
    "    # Secondary fallback: Check the content start\n",
    "    content_start: str = response.text[:300].lower()\n",
    "    return '<!doctype html>' in content_start or '<html' in content_start\n",
    "\n",
    "def crawl_domain(client: ScraperAPIClient, domain: str) -> (List[Dict[str, str]], List[str]):\n",
    "\n",
    "    base_url: str = f'https://{domain}'\n",
    "    ready_queue: List[str] = [base_url]\n",
    "    in_progress_set: Set[str] = set()\n",
    "    processed_set: Set[str] = set()\n",
    "    \n",
    "    domain_data: List[Dict[str, str]] = []\n",
    "    failed_sites: List[str] = []\n",
    "    \n",
    "    lock: Lock = Lock()\n",
    "\n",
    "    def crawl_site():\n",
    "        '''\n",
    "        1. Consume next url in queue to crawl, use Lock for memory safety,\n",
    "        2. In try-catch, make request using ScraperAPIClient, obtain raw html,\n",
    "        3. Extract text content and links from raw html, process links.\n",
    "        '''\n",
    "        with lock:\n",
    "            if not ready_queue:\n",
    "                return\n",
    "            url: str = ready_queue.pop(0)\n",
    "            in_progress_set.add(url)\n",
    "\n",
    "        try:\n",
    "            response = client.make_request(url=url, params={'retry': NUM_RETRIES})\n",
    "            \n",
    "            if is_html_response(response):\n",
    "                html: str = response.content\n",
    "                text_content: str = extract_text_content(html)\n",
    "                with lock:\n",
    "                    domain_data.append({\n",
    "                        'url': url,\n",
    "                        'text_content': text_content,\n",
    "                        'scrape_timestamp': datetime.now().isoformat()\n",
    "                    })\n",
    "                    new_links: List[str] = extract_new_links(html, domain, base_url, in_progress_set, processed_set)\n",
    "                    ready_queue.extend(new_links)\n",
    "                    logging.info(f'Success! Adding {len(new_links)} new links to queue...')\n",
    "            \n",
    "        except ScraperAPIException as e:\n",
    "            with lock:\n",
    "                failed_sites.append(url)\n",
    "            logging.error(e)\n",
    "\n",
    "        # Clean up by moving url from in-progress to processed set.\n",
    "        with lock:\n",
    "            processed_set.add(url)\n",
    "            in_progress_set.remove(url)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        while True:\n",
    "            with lock:\n",
    "                if not ready_queue and not in_progress_set:\n",
    "                    break\n",
    "            executor.submit(crawl_site)\n",
    "            sleep(0.1)\n",
    "\n",
    "    logging.info(f'Successfully crawled {len(domain_data)} sites from domain: {domain}')\n",
    "    \n",
    "    return domain_data, failed_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8a830-dcac-44b2-b209-47f1c2782afe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "domain: str = 'www.igs.com'\n",
    "domain_data, failed_sites = crawl_domain(client, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67268b2a-9e79-4bec-9558-adf7f942fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, HttpUrl, constr\n",
    "from datetime import datetime\n",
    "from typing import List, Optional\n",
    "\n",
    "class LocData(BaseModel):\n",
    "    loc: HttpUrl\n",
    "    lastmod: Optional[datetime] = None\n",
    "\n",
    "class SitemapData(BaseModel):\n",
    "    locs: List[LocData]\n",
    "\n",
    "class SiteData(BaseModel):\n",
    "    url: HttpUrl\n",
    "    src: Optional[HttpUrl]\n",
    "    text_content: constr(min_length=1)\n",
    "    scrape_timestamp: datetime\n",
    "    year_published: Optional[int] = None\n",
    "\n",
    "class DomainData(BaseModel):\n",
    "    sites: List[SiteData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3780a127-cc09-4538-bcfb-8803c0fe0975",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from lxml import etree\n",
    "\n",
    "SITEMAP_NAMESPACE = {'s': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "\n",
    "class EmptySitemapDataException(Exception):\n",
    "    '''No valid urls found in the requested domain's sitemap.xml.'''\n",
    "    pass\n",
    "\n",
    "def get_subfield(url_tag: etree._Element, field_name: str) -> Optional[str]:\n",
    "    field = url_tag.find(f's:{field_name}', SITEMAP_NAMESPACE)\n",
    "    return field.text if field is not None else None\n",
    "\n",
    "def get_sitemap_data(client: ScraperAPIClient, sitemap_url: str) -> List[Dict[str, str]]:\n",
    "    # Use 'make_request' instead of 'get' for byte str compatibility.\n",
    "    # i.e. w/ encoding declaration <?xml version=\"1.0\" encoding=\"UTF-8\"?> \n",
    "    response = client.make_request(url=sitemap_url, params={'retry': NUM_RETRIES})\n",
    "    root = etree.fromstring(response.content)\n",
    "    sitemap_data = []\n",
    "\n",
    "    for url in root.findall('.//s:url', SITEMAP_NAMESPACE):\n",
    "        loc = url.find('s:loc', SITEMAP_NAMESPACE)\n",
    "        if loc is not None:\n",
    "            url_data = {\n",
    "                'loc': loc.text,\n",
    "                'lastmod': get_subfield(url_tag=url, field_name='lastmod'),\n",
    "            }\n",
    "            sitemap_data.append(url_data)\n",
    "            # pprint(f'Success: Added URL data to sitemap dataset: {url_data}.')\n",
    "        else:\n",
    "            pprint(f'Warning: No <loc> tag found in URL data: {url}. Skipping...')\n",
    "    \n",
    "    if not sitemap_data:\n",
    "        raise EmptySitemapDataException\n",
    "\n",
    "    return sitemap_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25597972-0b87-46f5-a758-2d63a8525d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_output_filename(domain: str) -> str:\n",
    "    return f'{domain.replace('.', '_')}_domain_data.json'\n",
    "\n",
    "print(prepare_output_filename(domain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace3b363-d9ff-4834-ab06-0f546f87ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_filename = prepare_output_filename(domain)\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(domain_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c737068-c35a-46e1-a8b3-a07aced2f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556bde1b-f83e-4263-b462-480c3f403350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(domain_data)\n",
    "df['text_length'] = df['text_content'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e7300-e169-4bfd-b16a-0414e241b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    df, \n",
    "    x='text_length', \n",
    "    title='Distribution of Text Content Length',\n",
    "    labels={'text_length': 'Text Length (chars)'},\n",
    "    nbins=5000  # Adjust number of bins as needed\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"iframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538c14c-845f-437d-bb3e-9d32c5a196a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
